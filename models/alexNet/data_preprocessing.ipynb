{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing for AlexNet Model\n",
    "\n",
    "## Objective\n",
    "In this notebook, we will preprocess the human detection dataset to prepare it for training and evaluation on the AlexNet model. Preprocessing involves the following key steps:\n",
    "\n",
    "1. **Loading Images**: Load images from the dataset directory, which contains subdirectories for images with humans (`1`) and without humans (`0`).\n",
    "2. **Resizing**: Resize all images to 227x227 pixels to match the input size required by AlexNet.\n",
    "3. **Normalization**: Normalize pixel values to the range [0, 1] to improve training stability.\n",
    "4. **Dataset Splitting**: Split the dataset into training, validation, and test sets to ensure proper evaluation of the model's performance.\n",
    "\n",
    "This preprocessing is essential to standardize the input data and ensure compatibility with the AlexNet architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Functions for Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_images(data_dir, target_size=(227, 227)):\n",
    "    \"\"\"\n",
    "    Load and preprocess images from the dataset directory.\n",
    "    \n",
    "    Parameters:\n",
    "        data_dir (str): Path to the dataset directory.\n",
    "        target_size (tuple): Desired image size (width, height).\n",
    "        \n",
    "    Returns:\n",
    "        X (numpy array): Preprocessed images.\n",
    "        y (numpy array): Corresponding labels (1 for humans, 0 for no humans).\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    # Define subdirectories for classes\n",
    "    classes = ['0', '1']  # 0: No humans, 1: With humans\n",
    "\n",
    "    for label in classes:\n",
    "        class_dir = os.path.join(data_dir, label)\n",
    "        for file_name in os.listdir(class_dir):\n",
    "            file_path = os.path.join(class_dir, file_name)\n",
    "            image = cv2.imread(file_path)\n",
    "            if image is not None:  # Ensure valid image\n",
    "                image = cv2.resize(image, target_size)\n",
    "                images.append(image)\n",
    "                labels.append(int(label))\n",
    "\n",
    "    # Convert to NumPy arrays\n",
    "    X = np.array(images, dtype='float32') / 255.0  # Normalize\n",
    "    y = np.array(labels, dtype='int')\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(X, y, test_size=0.2, val_size=0.1):\n",
    "    \"\"\"\n",
    "    Split dataset into training, validation, and test sets.\n",
    "    \n",
    "    Parameters:\n",
    "        X (numpy array): Images.\n",
    "        y (numpy array): Labels.\n",
    "        test_size (float): Proportion of test data.\n",
    "        val_size (float): Proportion of validation data (from training set).\n",
    "        \n",
    "    Returns:\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=val_size, random_state=42)\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (662, 227, 227, 3), Validation set: (74, 227, 227, 3), Test set: (185, 227, 227, 3)\n",
      "Training labels: (662,), Validation labels: (74,), Test labels: (185,)\n"
     ]
    }
   ],
   "source": [
    "# Path to the dataset\n",
    "DATA_DIR = \"human_detection_dataset\"  # Replace with your dataset path\n",
    "\n",
    "# Load and preprocess images\n",
    "X, y = load_and_preprocess_images(DATA_DIR)\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_dataset(X, y)\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"Training set: {X_train.shape}, Validation set: {X_val.shape}, Test set: {X_test.shape}\")\n",
    "print(f\"Training labels: {y_train.shape}, Validation labels: {y_val.shape}, Test labels: {y_test.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
